{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bed3e0f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "import torch\n",
    "import gensim.downloader as api\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26ecac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc6b95f",
   "metadata": {},
   "source": [
    "# 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69183c5",
   "metadata": {},
   "source": [
    "#### The data given is read with the help of pandas read_csv using the compression gzip as data is compressed. Further I have used '\\t' as seperator to format data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f68e339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.tsv',sep='\\t',on_bad_lines='skip', header=0, quotechar='\"', dtype='unicode')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09fb47e",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings\n",
    "#### As mentioned below cell gets only 'review_body' and 'star_rating' columns from the whole data. Both the columns are merged and stored further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4106a314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           Love this, excellent sun block!!\n",
       "1          The great thing about this cream is that it do...\n",
       "2          Great Product, I'm 65 years old and this is al...\n",
       "3          I use them as shower caps & conditioning caps....\n",
       "4          This is my go-to daily sunblock. It leaves no ...\n",
       "                                 ...                        \n",
       "5094302    After watching my Dad struggle with his scisso...\n",
       "5094303    Like most sound machines, the sounds choices a...\n",
       "5094304    I bought this product because it indicated 30 ...\n",
       "5094305    We have used Oral-B products for 15 years; thi...\n",
       "5094306    I love this toothbrush. It's easy to use, and ...\n",
       "Name: review_body, Length: 5094307, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getData = data[['review_body','star_rating']]\n",
    "getData['review_body']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dee549f",
   "metadata": {},
   "source": [
    "#### Next the cell check for the NaN values for both the columns\n",
    "#### Review body has 400 NaN values and Star rating has 10 NaN values\n",
    "#### As the NaN values are less compared to the total dataset rows, I therefore dropped the NaN values from both the columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54eafe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_body    400\n",
      "star_rating     10\n",
      "dtype: int64\n",
      "review_body    0\n",
      "star_rating    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(getData.isnull().sum())\n",
    "getData=getData.dropna()\n",
    "print(getData.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170a9cb5",
   "metadata": {},
   "source": [
    " ## We form three classes and select 20000 reviews randomly from each class.\n",
    "#### label_class funtion helps to to add designated class to star_rating. In this a extra column is created with name class which stores the value of class with respect to star_rating\n",
    "#### Star_rating: 1, 2      Class: 1\n",
    "#### Star_rating: 3          Class: 2\n",
    "#### Star_rating: 4, 5      Class: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c4b99e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None \n",
    "def labelClass(rating):\n",
    "    if rating == \"1\" :\n",
    "          return 1\n",
    "    if rating == \"2\" :\n",
    "          return 1\n",
    "    if rating == \"3\" :\n",
    "          return 2\n",
    "    if rating == \"4\":\n",
    "          return 3\n",
    "    if rating  == \"5\":\n",
    "          return 3\n",
    "getData['class'] = getData['star_rating'].map(labelClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862d4ae0",
   "metadata": {},
   "source": [
    "#### Ramdom 20000 data rows from all three columns(review_body, star_rating, class) gets selected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8330f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classOne = getData.loc[getData['class'] == 1].sample(n=20000)\n",
    "classTwo = getData.loc[getData['class'] == 2].sample(n=20000)\n",
    "classThree = getData.loc[getData['class'] == 3].sample(n=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431824f3",
   "metadata": {},
   "source": [
    "#### The 20000 randomly slected data from all the three classes are further merged into one data providing the total rows of 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d18a6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>633312</th>\n",
       "      <td>As a female, I'm a a fan of male razors for a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>This is NOT Natural &amp; Not chemical Free.  Don'...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158261</th>\n",
       "      <td>not happy small model ,ill buy at bed and bath...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584493</th>\n",
       "      <td>Way too much vanilla for me. Even the wife sai...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051429</th>\n",
       "      <td>I have had two ER411 Nose and Ear Hair Groomer...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257980</th>\n",
       "      <td>Love this stuff! Makes my face super smooth. S...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3130340</th>\n",
       "      <td>I had some baby perfume in a bottle for my dau...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442853</th>\n",
       "      <td>People are saying that this shampoo leaves a f...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4836156</th>\n",
       "      <td>Started using twice a day and washing once a d...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577826</th>\n",
       "      <td>Great retinol product but more expensive than ...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review_body star_rating  class\n",
       "633312   As a female, I'm a a fan of male razors for a ...           1      1\n",
       "3990     This is NOT Natural & Not chemical Free.  Don'...           1      1\n",
       "2158261  not happy small model ,ill buy at bed and bath...           1      1\n",
       "1584493  Way too much vanilla for me. Even the wife sai...           2      1\n",
       "5051429  I have had two ER411 Nose and Ear Hair Groomer...           2      1\n",
       "...                                                    ...         ...    ...\n",
       "1257980  Love this stuff! Makes my face super smooth. S...           5      3\n",
       "3130340  I had some baby perfume in a bottle for my dau...           5      3\n",
       "1442853  People are saying that this shampoo leaves a f...           5      3\n",
       "4836156  Started using twice a day and washing once a d...           4      3\n",
       "577826   Great retinol product but more expensive than ...           5      3\n",
       "\n",
       "[60000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getRandomData = pd.concat([classOne, classTwo, classThree])\n",
    "getRandomData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b934270e",
   "metadata": {},
   "source": [
    "#### various data cleaning strategies are are applied to clean and process the review data\n",
    "#### 1) all the reviews are converted to lower case\n",
    "#### 2) HTML and URL's are removed from the reviews\n",
    "#### 3) Last contractions are performed on the review's this will change won’t → will not, I'm → i am and so on with the use of contraction library\n",
    "#### 4) Extra spaces are removed\n",
    "#### 5) Non alphabetical characters are also removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90585725",
   "metadata": {},
   "outputs": [],
   "source": [
    "getRandomData['review_body'] = getRandomData['review_body'].astype(str)\n",
    "\n",
    "getRandomData['review_body'] = getRandomData['review_body'].apply(str.lower)\n",
    "getRandomData['review_body'] = getRandomData['review_body'].apply(lambda x: re.sub(re.compile('<.*?>'), \" \", x))\n",
    "getRandomData['review_body'] = getRandomData['review_body'].apply(lambda x: re.sub(re.compile('http\\S+|https\\S+'),\" \", x))\n",
    "getRandomData['review_body'] = getRandomData['review_body'].apply(lambda x: re.sub(' +',' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb744fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contra(text):\n",
    "    words = []   \n",
    "    for word in text.split():\n",
    "        words.append(contractions.fix(word))  \n",
    "    word_text = ' '.join(words)\n",
    "    return word_text\n",
    "getRandomData['review_body'] = getRandomData['review_body'].apply(contra)\n",
    "getRandomData['review_body'] = getRandomData['review_body'].apply(lambda x: re.sub(re.compile(\"[^A-Za-z]\"),\" \",x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b237be2",
   "metadata": {},
   "source": [
    "# 2. Word Embedding (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0eb104",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38bfda0",
   "metadata": {},
   "source": [
    "#### loading word2vec-google-news-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b01631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e607588",
   "metadata": {},
   "source": [
    "#### pretrained examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4765b521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48333582"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"food\", \"rice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98079ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012386799"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"judge\", \"tedious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed34ed6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('polished', 0.43561863899230957),\n",
       " ('effortlessly', 0.3926153779029846),\n",
       " ('readily', 0.37599194049835205),\n",
       " ('instantly', 0.3723205626010895),\n",
       " ('inexpensively', 0.35900598764419556),\n",
       " ('quickness_explosiveness', 0.3580486476421356),\n",
       " ('polishes', 0.3550904393196106),\n",
       " ('quickly', 0.3545926809310913),\n",
       " ('swf_file', 0.3500930070877075),\n",
       " ('skils', 0.3500587046146393)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['polish', 'easily'], negative=['messy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc08ad31",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1e28fe",
   "metadata": {},
   "source": [
    "#### trianing word2vec model on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbf24b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSplitwords(review):\n",
    "    return review.split(' ')\n",
    "splitSentence = getRandomData['review_body'].apply(getSplitwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "366f220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainModel = Word2Vec(splitSentence, vector_size=300, window=13, min_count=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd18d102",
   "metadata": {},
   "source": [
    "#### example on our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d658665a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43362138"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainModel.wv.similarity(\"food\", \"rice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d2dee1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26440674"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainModel.wv.similarity(\"judge\", \"tedious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "586e3ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nail', 0.7342321276664734),\n",
       " ('nails', 0.6688219308853149),\n",
       " ('chip', 0.6668947339057922),\n",
       " ('polishes', 0.6637318730354309),\n",
       " ('file', 0.6354458332061768),\n",
       " ('chipping', 0.6278954148292542),\n",
       " ('coat', 0.6158057451248169),\n",
       " ('opi', 0.5834757089614868),\n",
       " ('manicure', 0.5627020597457886),\n",
       " ('acrylic', 0.5450944900512695)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainModel.wv.most_similar(positive=['polish', 'easily'], negative=['messy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e13bd",
   "metadata": {},
   "source": [
    "#### Q) What do you conclude from comparing vectors generated by yourself and the pretrained model?\n",
    "####       From the above examples taken it seems that the vectors generated by ourself (second part) gives better results compare to pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f10998",
   "metadata": {},
   "source": [
    "# 3. Simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f827d3df",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dd1d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = getRandomData['review_body']\n",
    "# y = getRandomData['class']\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "734b7a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, getRandomData['class'], test_size = 0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14832d37",
   "metadata": {},
   "source": [
    "#### using perceptron model with TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9464612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.64      0.65      0.64      3969\n",
      "           2       0.54      0.53      0.53      3986\n",
      "           3       0.71      0.72      0.72      4045\n",
      "\n",
      "    accuracy                           0.63     12000\n",
      "   macro avg       0.63      0.63      0.63     12000\n",
      "weighted avg       0.63      0.63      0.63     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfiDfPer = Perceptron(tol=1e-3, random_state=0)\n",
    "tfiDfPer.fit(X_train, y_train)\n",
    "y_pred = tfiDfPer.predict(X_test)\n",
    "perceptronCR = classification_report(y_test, y_pred, output_dict = True)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6d74761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report of Perceptron - TF-IDF\n",
      "Class        Precision            Recall             F1-score\n",
      "1      0.6431441486690106,   0.6452506928697405,   0.6441956986542574\n",
      "2      0.538972655251725,   0.5291018564977421,   0.5339916445119636\n",
      "3      0.7123020706455542,   0.722867737948084,   0.7175460122699386\n",
      "average 0.6314729581887631, 0.6324067624385221, 0.63191111847872\n",
      "Final accuracy 0.6328333333333334\n"
     ]
    }
   ],
   "source": [
    "print('classification report of Perceptron - TF-IDF')\n",
    "print('Class        Precision            Recall             F1-score')\n",
    "print('1'+\"      \"+ str(perceptronCR['1']['precision'])+\",   \"+str(perceptronCR['1']['recall'])+\",   \"+ str(perceptronCR['1']['f1-score']))\n",
    "print('2'+\"      \"+ str(perceptronCR['2']['precision'])+\",   \"+str(perceptronCR['2']['recall'])+\",   \"+ str(perceptronCR['2']['f1-score']))\n",
    "print('3'+\"      \"+ str(perceptronCR['3']['precision'])+\",   \"+str(perceptronCR['3']['recall'])+\",   \"+ str(perceptronCR['3']['f1-score']))\n",
    "print('average' +\" \"+str((perceptronCR['1']['precision']+perceptronCR['2']['precision']+perceptronCR['3']['precision'])/3)+\", \"+str((perceptronCR['1']['recall']+perceptronCR['2']['recall']+perceptronCR['3']['recall'])/3)+\", \"+str((perceptronCR['1']['f1-score']+perceptronCR['2']['f1-score']+perceptronCR['3']['f1-score'])/3))\n",
    "print('Final accuracy', perceptronCR['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37f2702",
   "metadata": {},
   "source": [
    "#### using SVM model with TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b892e67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.73      0.72      3969\n",
      "           2       0.61      0.59      0.60      3986\n",
      "           3       0.77      0.78      0.77      4045\n",
      "\n",
      "    accuracy                           0.70     12000\n",
      "   macro avg       0.70      0.70      0.70     12000\n",
      "weighted avg       0.70      0.70      0.70     12000\n",
      "\n",
      "Final accuracy 0.6976666666666667\n"
     ]
    }
   ],
   "source": [
    "tfiDf_SVM = LinearSVC(random_state = 0)\n",
    "tfiDf_SVM.fit(X_train,y_train)\n",
    "y_pred = tfiDf_SVM.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "tfidfsvmacc = classification_report(y_test, y_pred, output_dict = True)\n",
    "print('Final accuracy', tfidfsvmacc['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343d2be4",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e8b5d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "sentence = []\n",
    "\n",
    "for review in getRandomData['review_body']:\n",
    "    words = review.split(' ')\n",
    "    for word in words:\n",
    "        try:\n",
    "            embedding = trainModel.wv[word]\n",
    "        except KeyError:\n",
    "            embedding = np.zeros(300)\n",
    "        sentence.append(embedding)\n",
    "\n",
    "    sentence = np.array(sentence)\n",
    "    sentence_embedding = np.mean(sentence, axis=0)\n",
    "    embeddings.append(sentence_embedding)\n",
    "    sentence = []\n",
    "data = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4e75d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_train, X_w2v_test, y_w2v_train, y_w2v_test = train_test_split(data, getRandomData['class'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853c506a",
   "metadata": {},
   "source": [
    "#### using perceptron model with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f75ae800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.13      0.22      3963\n",
      "           2       0.41      0.89      0.56      4014\n",
      "           3       0.77      0.49      0.60      4023\n",
      "\n",
      "    accuracy                           0.50     12000\n",
      "   macro avg       0.65      0.50      0.46     12000\n",
      "weighted avg       0.65      0.50      0.46     12000\n",
      "\n",
      "Final accuracy 0.504\n"
     ]
    }
   ],
   "source": [
    "w2vPer = Perceptron(tol=1e-3, random_state=0)\n",
    "w2vPer.fit(X_w2v_train, y_w2v_train)\n",
    "y_pred = w2vPer.predict(X_w2v_test)\n",
    "perceptronCR = classification_report(y_w2v_test, y_pred, output_dict = True)\n",
    "print(classification_report(y_w2v_test, y_pred))\n",
    "w2vpacc = classification_report(y_w2v_test, y_pred, output_dict = True)\n",
    "print('Final accuracy', w2vpacc['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef363e3f",
   "metadata": {},
   "source": [
    "#### using SVM model with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83177502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.72      0.69      3963\n",
      "           2       0.62      0.56      0.59      4014\n",
      "           3       0.74      0.74      0.74      4023\n",
      "\n",
      "    accuracy                           0.67     12000\n",
      "   macro avg       0.67      0.67      0.67     12000\n",
      "weighted avg       0.67      0.67      0.67     12000\n",
      "\n",
      "Final accuracy 0.6746666666666666\n"
     ]
    }
   ],
   "source": [
    "w2v_SVM = LinearSVC(random_state = 0)\n",
    "w2v_SVM.fit(X_w2v_train,y_w2v_train)\n",
    "y_pred = w2v_SVM.predict(X_w2v_test)\n",
    "print(classification_report(y_w2v_test, y_pred))\n",
    "w2vsvmacc = classification_report(y_w2v_test, y_pred, output_dict = True)\n",
    "print('Final accuracy', w2vsvmacc['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c549e9",
   "metadata": {},
   "source": [
    "#### Q) What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained Word2Vec features)?\n",
    "#### Both the Perceptron and SVM models perform better with TF-IDF than with Word2Vec embeddings. This may be due to the fact that the Word2Vec model used in this study was trained on Google News articles, which may not include words commonly used in reviews. Additionally, the Word2Vec embeddings may not accurately represent the importance of certain words in our specific corpus, whereas TF-IDF is able to reflect the importance of words within our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991904d3",
   "metadata": {},
   "source": [
    "# 4. Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f598b",
   "metadata": {},
   "source": [
    "#### creating one hot encoding on class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f86090e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = copy.deepcopy(getRandomData[\"class\"])\n",
    "ohe = OneHotEncoder(sparse = False)\n",
    "ohe.fit(np.asarray([[1],[2],[3]]))\n",
    "yohe = ohe.transform(y.values.reshape((60000,1)))\n",
    "\n",
    "word2VecSet = set(trainModel.wv.key_to_index.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa59de6",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05d9c1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "sentence = []\n",
    "\n",
    "for review in getRandomData['review_body']:\n",
    "    words = review.split(' ')\n",
    "    for word in words:\n",
    "        try:\n",
    "            embedding = trainModel.wv[word]\n",
    "        except KeyError:\n",
    "            embedding = np.zeros(300)\n",
    "        sentence.append(embedding)\n",
    "\n",
    "    sentence = np.array(sentence)\n",
    "    sentence_embedding = np.mean(sentence, axis=0)\n",
    "    embeddings.append(sentence_embedding)\n",
    "    sentence = []\n",
    "data = np.array(embeddings)\n",
    "\n",
    "Xw2v_train, Xw2v_test, Yw2v_train, Yw2v_test = train_test_split(data, yohe, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd69dd0",
   "metadata": {},
   "source": [
    "#### converting the NumPy arrays Xw2v_train and Yw2v_train into PyTorch tensors and creating a dataset and data loader for efficient batching of training data during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "520d078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xw2v_train = torch.FloatTensor(Xw2v_train).to(device)\n",
    "Yw2v_train = torch.from_numpy(Yw2v_train).to(device)\n",
    "train = torch.utils.data.TensorDataset(Xw2v_train, Yw2v_train)\n",
    "trainLoader = torch.utils.data.DataLoader(train, batch_size=32, num_workers=0, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c31282e",
   "metadata": {},
   "source": [
    "#### converting the NumPy arrays Xw2v_test and Yw2v_test into PyTorch tensors and creating a dataset and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a42680de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xw2v_test = torch.FloatTensor(Xw2v_test).to(device)\n",
    "Yw2v_test = torch.from_numpy(Yw2v_test).to(device)\n",
    "test = torch.utils.data.TensorDataset(Xw2v_test, Yw2v_test)\n",
    "testLoader = torch.utils.data.DataLoader(test, batch_size=32, num_workers=0, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477bf2db",
   "metadata": {},
   "source": [
    "#### defines a Feedforward Neural Network (FNN) model using PyTorch's neural network module, where the model has three linear layers and ReLU activation functions. It then instantiates the model and assigns it to a variable named fnnModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97632431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.fc3 = nn.Linear(10, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 300)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "fnnModel = FNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "389af8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN(\n",
      "  (fc1): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "fnnModel = fnnModel.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(fnnModel.parameters(), lr=0.0001)\n",
    "print(fnnModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b900a0",
   "metadata": {},
   "source": [
    "#### defines a function named runModel that trains and evaluates a PyTorch model for a certain number of epochs using the specified optimizer, loss function, and training and testing data loaders. It also saves the model's state dictionary if the validation loss improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0e73b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(model, optimizer, criterion, trainLoader, testLoader):\n",
    "    epochs = 50\n",
    "    validationLossMin = np.Inf\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        trainingLoss = 0.0\n",
    "        validationLoss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for data, target in trainLoader:\n",
    "            optimizer.zero_grad()\n",
    "            getOutput = model(data)\n",
    "            getOutput = getOutput.to(device)\n",
    "            getLoss = criterion(getOutput, target)\n",
    "            getLoss.backward()\n",
    "            optimizer.step()\n",
    "            trainingLoss += getLoss.item()*data.size(0)\n",
    "\n",
    "        model.eval() \n",
    "        with torch.no_grad():\n",
    "            for data, target in testLoader:\n",
    "                getOutput = model(data)\n",
    "                getOutput = getOutput.to(device)\n",
    "                getLoss = criterion(getOutput, target)\n",
    "                validationLoss += getLoss.item()*data.size(0)\n",
    "\n",
    "        trainingLoss = trainingLoss/len(trainLoader.dataset)\n",
    "        validationLoss = validationLoss/len(testLoader.dataset)\n",
    "        print('Epoch:', epoch+1)\n",
    "        print('Training loss', trainingLoss)\n",
    "        print('Validation loss:', validationLoss)\n",
    "        print('____________________________________________________________________________________________________________')\n",
    "\n",
    "        if validationLoss <= validationLossMin:\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "            validationLossMin = validationLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ec53576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training loss 0.9117901773699463\n",
      "Validation loss: 0.8165627371261363\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 2\n",
      "Training loss 0.7940597255324368\n",
      "Validation loss: 0.7854675148968235\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 3\n",
      "Training loss 0.7732538627692338\n",
      "Validation loss: 0.770903786336421\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 4\n",
      "Training loss 0.7627920664017317\n",
      "Validation loss: 0.7646510052934755\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 5\n",
      "Training loss 0.755237314068739\n",
      "Validation loss: 0.7601361087185409\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 6\n",
      "Training loss 0.7498612696126608\n",
      "Validation loss: 0.7568162430433052\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 7\n",
      "Training loss 0.744795577623064\n",
      "Validation loss: 0.7513501551490723\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 8\n",
      "Training loss 0.7408319776225204\n",
      "Validation loss: 0.7494931003062132\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 9\n",
      "Training loss 0.7365707223691764\n",
      "Validation loss: 0.7463055639220305\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 10\n",
      "Training loss 0.7332475389788039\n",
      "Validation loss: 0.7437375116480398\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 11\n",
      "Training loss 0.7294259147394684\n",
      "Validation loss: 0.740412067229714\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 12\n",
      "Training loss 0.7264013801423692\n",
      "Validation loss: 0.7422577634108748\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 13\n",
      "Training loss 0.7238326146252536\n",
      "Validation loss: 0.7389963919809815\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 14\n",
      "Training loss 0.7207142996171181\n",
      "Validation loss: 0.7372387894269923\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 15\n",
      "Training loss 0.7182432751130328\n",
      "Validation loss: 0.7346448899490885\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 16\n",
      "Training loss 0.7159344325661138\n",
      "Validation loss: 0.7351935100158637\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 17\n",
      "Training loss 0.713566611903391\n",
      "Validation loss: 0.7354359986694956\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 18\n",
      "Training loss 0.7117261323955184\n",
      "Validation loss: 0.7306913229831165\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 19\n",
      "Training loss 0.7093144330552241\n",
      "Validation loss: 0.7300420890779206\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 20\n",
      "Training loss 0.7068610341060084\n",
      "Validation loss: 0.7292759712453699\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 21\n",
      "Training loss 0.7048229120880023\n",
      "Validation loss: 0.7274329169555567\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 22\n",
      "Training loss 0.7030416920319342\n",
      "Validation loss: 0.7259146093205878\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 23\n",
      "Training loss 0.7011755634761151\n",
      "Validation loss: 0.7280428963340867\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 24\n",
      "Training loss 0.6986989693992703\n",
      "Validation loss: 0.7271787353564577\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 25\n",
      "Training loss 0.6970457725978947\n",
      "Validation loss: 0.7251975811652107\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 26\n",
      "Training loss 0.6950646390257886\n",
      "Validation loss: 0.7315418338135302\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 27\n",
      "Training loss 0.6937481163079343\n",
      "Validation loss: 0.724020908808098\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 28\n",
      "Training loss 0.6920924102983942\n",
      "Validation loss: 0.7224558003758721\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 29\n",
      "Training loss 0.6901334096243299\n",
      "Validation loss: 0.7222837740776279\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 30\n",
      "Training loss 0.6883222522823198\n",
      "Validation loss: 0.7219788562081692\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 31\n",
      "Training loss 0.6868003329603537\n",
      "Validation loss: 0.7227512905105565\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 32\n",
      "Training loss 0.6857180726839907\n",
      "Validation loss: 0.7225899381262401\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 33\n",
      "Training loss 0.6835649127224884\n",
      "Validation loss: 0.7209213886190846\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 34\n",
      "Training loss 0.6824018587564242\n",
      "Validation loss: 0.7209219164741735\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 35\n",
      "Training loss 0.6804976851330612\n",
      "Validation loss: 0.7232854842243954\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 36\n",
      "Training loss 0.6791834563643631\n",
      "Validation loss: 0.7226045705574118\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 37\n",
      "Training loss 0.6776094628023214\n",
      "Validation loss: 0.7223124502379336\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 38\n",
      "Training loss 0.6765386723316879\n",
      "Validation loss: 0.7223309109902063\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 39\n",
      "Training loss 0.6749849507516156\n",
      "Validation loss: 0.7220625879707926\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 40\n",
      "Training loss 0.6735956906417135\n",
      "Validation loss: 0.7210071191691726\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 41\n",
      "Training loss 0.6721243272566171\n",
      "Validation loss: 0.7219098514037784\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 42\n",
      "Training loss 0.6705347672796391\n",
      "Validation loss: 0.7210150736537859\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 43\n",
      "Training loss 0.6694217528820251\n",
      "Validation loss: 0.7197647232274685\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 44\n",
      "Training loss 0.6684639160118567\n",
      "Validation loss: 0.7202197767101703\n",
      "____________________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45\n",
      "Training loss 0.6669488105342062\n",
      "Validation loss: 0.7203278188878078\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 46\n",
      "Training loss 0.6657089521477404\n",
      "Validation loss: 0.7210484327211889\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 47\n",
      "Training loss 0.6641907483216903\n",
      "Validation loss: 0.7236430869176038\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 48\n",
      "Training loss 0.6633269144826736\n",
      "Validation loss: 0.7213868788516847\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 49\n",
      "Training loss 0.662268445276403\n",
      "Validation loss: 0.7201395249270451\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 50\n",
      "Training loss 0.6606404103151847\n",
      "Validation loss: 0.7204235745607148\n",
      "____________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "runModel(fnnModel, optimizer, criterion, trainLoader, testLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d91bf81",
   "metadata": {},
   "source": [
    "#### load efficient model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "daf88c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnnModel.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ccafbf",
   "metadata": {},
   "source": [
    "#### running model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d745e3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.70      0.70      3963\n",
      "           2       0.61      0.63      0.62      4014\n",
      "           3       0.78      0.74      0.76      4023\n",
      "\n",
      "    accuracy                           0.69     12000\n",
      "   macro avg       0.69      0.69      0.69     12000\n",
      "weighted avg       0.70      0.69      0.69     12000\n",
      "\n",
      "Final accuracy 0.6928333333333333\n"
     ]
    }
   ],
   "source": [
    "getPred = []\n",
    "getLabel = []\n",
    "\n",
    "fnnModel.eval()\n",
    "with torch.no_grad():\n",
    "    for data, target in testLoader:\n",
    "        output = fnnModel(data)\n",
    "\n",
    "        getLabel.append(target.cpu().detach().numpy())\n",
    "\n",
    "        _, predicted = torch.max(output, 1) \n",
    "        getPred.append(predicted.cpu().detach().numpy())\n",
    "\n",
    "predictions = np.array(getPred)\n",
    "\n",
    "finalLabels = []\n",
    "for batch in getLabel:\n",
    "    t = []\n",
    "    for b in batch:\n",
    "        t.append(np.argmax(b))\n",
    "    finalLabels.append(t)\n",
    "np.shape(finalLabels) \n",
    "\n",
    "finalLabels = np.array(finalLabels)\n",
    "predictions = list(predictions)\n",
    "\n",
    "finalLabelss = []\n",
    "for f in finalLabels:\n",
    "    for d in f:\n",
    "        finalLabelss.append(d)\n",
    "\n",
    "predictionss = []\n",
    "for f in predictions:\n",
    "    for d in f:\n",
    "        predictionss.append(d)\n",
    "\n",
    "print(classification_report(finalLabelss, predictionss, target_names = [\"1\",\"2\",\"3\"]))\n",
    "fnnacc = classification_report(finalLabelss, predictionss, target_names = [\"1\",\"2\",\"3\"], output_dict = True)\n",
    "print('Final accuracy', fnnacc['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b36976",
   "metadata": {},
   "source": [
    "###############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868e6892",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3329696f",
   "metadata": {},
   "source": [
    "#### a convolutional feedforward neural network with three fully connected layers. The input size to the network is 3000, and the activation function used in each layer is ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dad1b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class FNNCon(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FNNCon, self).__init__()\n",
    "        self.fc1 = nn.Linear(3000, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.fc3 = nn.Linear(10, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3000)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e575d47",
   "metadata": {},
   "source": [
    " #### list of feature vectors for text data using word embeddings from a trained Word2Vec model. Each feature vector has a length of 3000 and contains the concatenated embeddings of the first 10 words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5c98ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for sntnc in getRandomData['review_body']:\n",
    "    wrds = sntnc.split(' ')\n",
    "    tmp = np.array([trainModel.wv[w] for w in wrds[:10] if w in word2VecSet])\n",
    "    if len(tmp) == 0:\n",
    "        tmp = np.zeros((1,300))\n",
    "    wrds = np.concatenate(tmp, axis = 0)     \n",
    "    if len(wrds)<3000:\n",
    "        wrds = np.concatenate([wrds, np.zeros(3000-len(wrds))])\n",
    "    X.append(wrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03bd61f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xw2v_train, Xw2v_test, Yw2v_train, Yw2v_test = train_test_split(X, yohe, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5437a20",
   "metadata": {},
   "source": [
    "#### converting the NumPy arrays Xw2v_train and Yw2v_train into PyTorch tensors and creating a dataset and data loader for efficient batching of training data during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ad41e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xw2v_train = torch.FloatTensor(Xw2v_train).to(device)\n",
    "Yw2v_train = torch.from_numpy(Yw2v_train).to(device)\n",
    "train = torch.utils.data.TensorDataset(Xw2v_train, Yw2v_train)\n",
    "trainLoader = torch.utils.data.DataLoader(train, batch_size=256, num_workers=0, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7fb706",
   "metadata": {},
   "source": [
    "#### converting the NumPy arrays Xw2v_test and Yw2v_test into PyTorch tensors and creating a dataset and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c19b1517",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xw2v_test = torch.FloatTensor(Xw2v_test).to(device)\n",
    "Yw2v_test = torch.from_numpy(Yw2v_test).to(device)\n",
    "test = torch.utils.data.TensorDataset(Xw2v_test, Yw2v_test)\n",
    "testLoader = torch.utils.data.DataLoader(test, batch_size=256, num_workers=0, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "185cad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnnconModel = FNNCon().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(fnnconModel.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50ecfb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training loss 1.0310021750617597\n",
      "Validation loss: 1.0120739625386874\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 2\n",
      "Training loss 1.0036538439338305\n",
      "Validation loss: 1.0109601649350564\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 3\n",
      "Training loss 0.9951491308659879\n",
      "Validation loss: 1.0089335459221038\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 4\n",
      "Training loss 0.9854862428687367\n",
      "Validation loss: 1.023343403830399\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 5\n",
      "Training loss 0.9777700959140924\n",
      "Validation loss: 1.0135072564483367\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 6\n",
      "Training loss 0.9737277022718933\n",
      "Validation loss: 1.0146420966739365\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 7\n",
      "Training loss 0.9784091694974465\n",
      "Validation loss: 1.0458211660810763\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 8\n",
      "Training loss 0.9794440781239456\n",
      "Validation loss: 1.0231864384853309\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 9\n",
      "Training loss 0.9592153315918407\n",
      "Validation loss: 1.0273787855710534\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 10\n",
      "Training loss 0.9526289717454373\n",
      "Validation loss: 1.0308136152181275\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 11\n",
      "Training loss 0.9387644932328403\n",
      "Validation loss: 1.0405589837586793\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 12\n",
      "Training loss 0.927632860221642\n",
      "Validation loss: 1.0466046424630324\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 13\n",
      "Training loss 0.9217388780115683\n",
      "Validation loss: 1.0558596141656105\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 14\n",
      "Training loss 0.918525741307152\n",
      "Validation loss: 1.0742613604466882\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 15\n",
      "Training loss 0.9139850309235077\n",
      "Validation loss: 1.061979154117869\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 16\n",
      "Training loss 0.9106186115597663\n",
      "Validation loss: 1.1065107829507874\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 17\n",
      "Training loss 0.8993875683903831\n",
      "Validation loss: 1.0849345772433736\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 18\n",
      "Training loss 0.9000284742321304\n",
      "Validation loss: 1.0881451392605388\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 19\n",
      "Training loss 0.8967328758781449\n",
      "Validation loss: 1.0479276099303192\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 20\n",
      "Training loss 0.889175565897983\n",
      "Validation loss: 1.0614698911553606\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 21\n",
      "Training loss 0.8861273555208374\n",
      "Validation loss: 1.1108369702984267\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 22\n",
      "Training loss 0.8856010478338067\n",
      "Validation loss: 1.115152750750184\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 23\n",
      "Training loss 0.8877999038556145\n",
      "Validation loss: 1.1219974270899211\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 24\n",
      "Training loss 0.8792851703227414\n",
      "Validation loss: 1.0843991321270612\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 25\n",
      "Training loss 0.8745646848300691\n",
      "Validation loss: 1.2367895148586638\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 26\n",
      "Training loss 0.8769890710361801\n",
      "Validation loss: 1.0950400813708698\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 27\n",
      "Training loss 0.8818626849716452\n",
      "Validation loss: 1.1046075543047882\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 28\n",
      "Training loss 0.8831862598811017\n",
      "Validation loss: 1.1482594259143464\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 29\n",
      "Training loss 0.8786714678253897\n",
      "Validation loss: 1.215665171381161\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 30\n",
      "Training loss 0.8766281406440364\n",
      "Validation loss: 1.195058702639472\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 31\n",
      "Training loss 0.874991717829428\n",
      "Validation loss: 1.2014039133715715\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 32\n",
      "Training loss 0.8704345748145662\n",
      "Validation loss: 1.2100659965161584\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 33\n",
      "Training loss 0.8763781124975535\n",
      "Validation loss: 1.2374551395605553\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 34\n",
      "Training loss 0.872446479602498\n",
      "Validation loss: 1.1645087658649527\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 35\n",
      "Training loss 0.8789668846439672\n",
      "Validation loss: 1.127401263674284\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 36\n",
      "Training loss 0.8734865141597832\n",
      "Validation loss: 1.2081910167184204\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 37\n",
      "Training loss 0.8752393788255214\n",
      "Validation loss: 1.132961707972558\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 38\n",
      "Training loss 0.8700725182261608\n",
      "Validation loss: 1.2000507843852015\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 39\n",
      "Training loss 0.8682055626308144\n",
      "Validation loss: 1.1256258975668958\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 40\n",
      "Training loss 0.8642592939602628\n",
      "Validation loss: 1.283596592068476\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 41\n",
      "Training loss 0.8679558495018451\n",
      "Validation loss: 1.1467709042089846\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 42\n",
      "Training loss 0.8659830277152165\n",
      "Validation loss: 1.0829637379836594\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 43\n",
      "Training loss 0.8616397119227863\n",
      "Validation loss: 1.2002207533842104\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 44\n",
      "Training loss 0.8648954285857364\n",
      "Validation loss: 1.1981936763531813\n",
      "____________________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45\n",
      "Training loss 0.8579790318528385\n",
      "Validation loss: 1.3094268685056796\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 46\n",
      "Training loss 0.8561022095031517\n",
      "Validation loss: 1.3672401818784035\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 47\n",
      "Training loss 0.8560032369356871\n",
      "Validation loss: 1.195804426387093\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 48\n",
      "Training loss 0.8633322985133656\n",
      "Validation loss: 1.3202628670939496\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 49\n",
      "Training loss 0.8624824269136165\n",
      "Validation loss: 1.1283750085132587\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 50\n",
      "Training loss 0.8614072011414444\n",
      "Validation loss: 1.2945465465847736\n",
      "____________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "runModel(fnnconModel, optimizer, criterion, trainLoader, testLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47126a3e",
   "metadata": {},
   "source": [
    "#### load efficient model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd019c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.37      0.42      3963\n",
      "           2       0.41      0.60      0.49      4014\n",
      "           3       0.68      0.55      0.61      4023\n",
      "\n",
      "    accuracy                           0.51     12000\n",
      "   macro avg       0.53      0.51      0.51     12000\n",
      "weighted avg       0.53      0.51      0.51     12000\n",
      "\n",
      "Final accuracy 0.5068333333333334\n"
     ]
    }
   ],
   "source": [
    "getPred = []\n",
    "getLabel = []\n",
    "\n",
    "fnnconModel.eval()\n",
    "with torch.no_grad():\n",
    "    for data, target in testLoader:\n",
    "        output = fnnconModel(data)\n",
    "        \n",
    "        getLabel.append(target.cpu().detach().numpy())\n",
    "    \n",
    "        _, predicted = torch.max(output, 1) \n",
    "        getPred.append(predicted.cpu().detach().numpy())\n",
    "\n",
    "predictions = np.array(getPred)\n",
    "\n",
    "finalLabels = []\n",
    "for batch in getLabel:\n",
    "    t = []\n",
    "    for b in batch:\n",
    "        t.append(np.argmax(b))\n",
    "    finalLabels.append(t)\n",
    "\n",
    "finalLabels = np.array(finalLabels)\n",
    "predictions = list(predictions)\n",
    "\n",
    "finalLabelss = []\n",
    "for f in finalLabels:\n",
    "    for d in f:\n",
    "        finalLabelss.append(d)\n",
    "\n",
    "predictionss = []\n",
    "for f in predictions:\n",
    "    for d in f:\n",
    "        predictionss.append(d)\n",
    "\n",
    "print(classification_report(finalLabelss, predictionss, target_names = [\"1\",\"2\",\"3\"]))\n",
    "fnnconacc = classification_report(finalLabelss, predictionss, target_names = [\"1\",\"2\",\"3\"], output_dict = True)\n",
    "print('Final accuracy', fnnconacc['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ff2537",
   "metadata": {},
   "source": [
    "#### Q) What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section.\n",
    "#### The accuracy scores obtained by taking the mean outperform those generated by concatenation. One reason for this could be that taking the mean helps to capture the overall sentiment of the entire review, whereas the contribution of the first 10 words of a sentence may not necessarily be a strong indicator of sentiment and thus less useful for classification purposes.\n",
    "#### Also TF-IDF SVM performs slightly better than FNN model part (a) as reviews can often contain specific keywords and phrases that can be easily captured by the TF-IDF SVM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612ee9bb",
   "metadata": {},
   "source": [
    "##########################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f14e78",
   "metadata": {},
   "source": [
    "# 5. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa629bfa",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d463e4f1",
   "metadata": {},
   "source": [
    "#### an RNN neural network model with a single RNN layer, followed by a fully connected layer for classification. The RNN layer takes a sequence of input vectors with 300 features, produces a sequence of hidden states with 20 features, and the final hidden state is fed to the fully connected layer to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4edb2c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(300, 20, batch_first = True, nonlinearity='relu')   \n",
    "        self.fc = nn.Linear(20, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, hidden = self.rnn(x)\n",
    "        x = self.fc(x[:,-1,:])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deb0c47",
   "metadata": {},
   "source": [
    "####  a list of 20-dimensional word embeddings for each sentence in the 'review_body'. If a sentence has fewer than 20 words, it pads the embedding vector with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6aa80d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for sntnc in getRandomData['review_body']:\n",
    "    wrds = sntnc.split(' ')\n",
    "    if len(wrds)<20:\n",
    "        t = np.array([trainModel.wv[w] for w in wrds if w in word2VecSet])\n",
    "    else: \n",
    "        t = np.array([trainModel.wv[w] for w in wrds[:20] if w in word2VecSet])\n",
    "    if len(t) == 0:\n",
    "        wrds = np.zeros((20,300))\n",
    "    elif len(t)<20:\n",
    "        wrds = np.concatenate([t,np.zeros((20-len(t), 300))])\n",
    "    else: \n",
    "        wrds = t\n",
    "    X.append(wrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f975624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xw2v_train, Xw2v_test, Yw2v_train, Yw2v_test = train_test_split(X, yohe, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e7b048",
   "metadata": {},
   "source": [
    "#### converting the NumPy arrays Xw2v_train and Yw2v_train into PyTorch tensors and creating a dataset and data loader for efficient batching of training data during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bdaefaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xw2v_train = torch.FloatTensor(Xw2v_train).to(device)\n",
    "Yw2v_train = torch.from_numpy(Yw2v_train).to(device)\n",
    "train = torch.utils.data.TensorDataset(Xw2v_train, Yw2v_train)\n",
    "trainLoader = torch.utils.data.DataLoader(train, batch_size=32, num_workers=0, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0980f4e",
   "metadata": {},
   "source": [
    "#### converting the NumPy arrays Xw2v_test and Yw2v_test into PyTorch tensors and creating a dataset and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5db48b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xw2v_test = torch.FloatTensor(Xw2v_test).to(device)\n",
    "Yw2v_test = torch.from_numpy(Yw2v_test).to(device)\n",
    "test = torch.utils.data.TensorDataset(Xw2v_test, Yw2v_test)\n",
    "testLoader = torch.utils.data.DataLoader(test, batch_size=32, num_workers=0, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c3b59ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnModel = RNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(rnnModel.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c563131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training loss 1.0762579313580305\n",
      "Validation loss: 0.9707867726506665\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 2\n",
      "Training loss 0.9056449628887037\n",
      "Validation loss: 0.9097219310229605\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 3\n",
      "Training loss 0.8665094862467828\n",
      "Validation loss: 0.8513017264061419\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 4\n",
      "Training loss 0.8382157579871922\n",
      "Validation loss: 0.8506305748424391\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 5\n",
      "Training loss 0.8271501441389467\n",
      "Validation loss: 0.8412165344179806\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 6\n",
      "Training loss 0.8194644876614843\n",
      "Validation loss: 0.8378830165569767\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 7\n",
      "Training loss 0.8153055359042519\n",
      "Validation loss: 0.8287114498485583\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 8\n",
      "Training loss 0.8086377065137142\n",
      "Validation loss: 0.8343867627246825\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 9\n",
      "Training loss 0.8046818744099361\n",
      "Validation loss: 0.8159191817443255\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 10\n",
      "Training loss 0.7999303467546224\n",
      "Validation loss: 0.8153971808570847\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 11\n",
      "Training loss 0.7964347886963878\n",
      "Validation loss: 0.8181690148243167\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 12\n",
      "Training loss 0.7936642703119344\n",
      "Validation loss: 0.8121216893050975\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 13\n",
      "Training loss 0.7943097944127093\n",
      "Validation loss: 0.816474940780492\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 14\n",
      "Training loss 0.7890381091305817\n",
      "Validation loss: 0.8154882473641992\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 15\n",
      "Training loss 0.7958754099543974\n",
      "Validation loss: 0.8151748209773092\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 16\n",
      "Training loss 0.7819519657690652\n",
      "Validation loss: 0.8239351921350526\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 17\n",
      "Training loss 0.7881310121790372\n",
      "Validation loss: 0.8152610854911339\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 18\n",
      "Training loss 0.7776516535039515\n",
      "Validation loss: 0.822996356021842\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 19\n",
      "Training loss 0.7797187570489592\n",
      "Validation loss: 0.8120221225764823\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 20\n",
      "Training loss 0.7768012961240351\n",
      "Validation loss: 0.8195629444582799\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 21\n",
      "Training loss 0.7762174675060275\n",
      "Validation loss: 0.8072319491228419\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 22\n",
      "Training loss 0.7766113159420542\n",
      "Validation loss: 0.818859139699062\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 23\n",
      "Training loss 0.7736761131969427\n",
      "Validation loss: 0.8080264526486086\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 24\n",
      "Training loss 0.7703581949352186\n",
      "Validation loss: 0.8130834707239235\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 25\n",
      "Training loss 0.7712743716439933\n",
      "Validation loss: 0.826636774077592\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 26\n",
      "Training loss 0.7684582730335836\n",
      "Validation loss: 0.8036098805780202\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 27\n",
      "Training loss 0.7672370021450886\n",
      "Validation loss: 0.828841964987417\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 28\n",
      "Training loss 0.7675654004025859\n",
      "Validation loss: 0.81521302927332\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 29\n",
      "Training loss 0.7669247414357223\n",
      "Validation loss: 0.817965299973502\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 30\n",
      "Training loss 0.7686505644681876\n",
      "Validation loss: 0.8317006834617835\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 31\n",
      "Training loss 0.7683174354171497\n",
      "Validation loss: 0.8292052477580913\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 32\n",
      "Training loss 0.7690412639101563\n",
      "Validation loss: 0.8007342147492215\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 33\n",
      "Training loss 0.7649163663498056\n",
      "Validation loss: 0.8101737655116109\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 34\n",
      "Training loss 0.7667411313304807\n",
      "Validation loss: 0.814662146147343\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 35\n",
      "Training loss 0.7630879518095668\n",
      "Validation loss: 0.7971506183448777\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 36\n",
      "Training loss 0.7600878521740181\n",
      "Validation loss: 0.8142146232190522\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 37\n",
      "Training loss 0.7598760379416978\n",
      "Validation loss: 0.8070550618802711\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 38\n",
      "Training loss 0.7588786275168286\n",
      "Validation loss: 0.8366610581690038\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 39\n",
      "Training loss 0.7607182823269262\n",
      "Validation loss: 0.8051480573159691\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 40\n",
      "Training loss 0.7579220106034987\n",
      "Validation loss: 0.826610978325208\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 41\n",
      "Training loss 0.7585342444284564\n",
      "Validation loss: 0.8019584895619579\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 42\n",
      "Training loss 0.7564879500761983\n",
      "Validation loss: 0.8156747131048447\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 43\n",
      "Training loss 0.7550543694665748\n",
      "Validation loss: 0.8050173610155557\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 44\n",
      "Training loss 0.7568115751436598\n",
      "Validation loss: 0.8032687206060792\n",
      "____________________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45\n",
      "Training loss 0.7557280864718944\n",
      "Validation loss: 0.808660650354291\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 46\n",
      "Training loss 0.7552817032481565\n",
      "Validation loss: 0.8069484932398137\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 47\n",
      "Training loss 0.7553899678323378\n",
      "Validation loss: 0.8119224169796968\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 48\n",
      "Training loss 0.7536821404916897\n",
      "Validation loss: 0.8183656359112647\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 49\n",
      "Training loss 0.7537020936881862\n",
      "Validation loss: 0.8114003679368142\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 50\n",
      "Training loss 0.7525758197305319\n",
      "Validation loss: 0.8162620572931846\n",
      "____________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "runModel(rnnModel, optimizer, criterion, trainLoader, testLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f215b6f8",
   "metadata": {},
   "source": [
    "#### load efficient model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b175d315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.69      0.64      3963\n",
      "           2       0.59      0.43      0.49      4014\n",
      "           3       0.67      0.77      0.72      4023\n",
      "\n",
      "    accuracy                           0.63     12000\n",
      "   macro avg       0.62      0.63      0.62     12000\n",
      "weighted avg       0.62      0.63      0.62     12000\n",
      "\n",
      "Final accuracy 0.6274166666666666\n"
     ]
    }
   ],
   "source": [
    "getPred = []\n",
    "getLabels = []\n",
    "\n",
    "rnnModel.eval()\n",
    "with torch.no_grad():\n",
    "    for data, target in testLoader:\n",
    "        output = rnnModel(data)\n",
    "        \n",
    "        getLabels.append(target.cpu().detach().numpy())\n",
    "        _, predicted = torch.max(output, 1) \n",
    "        getPred.append(predicted.cpu().detach().numpy())\n",
    "\n",
    "predictions = np.array(getPred)\n",
    "\n",
    "finalLabels = []\n",
    "for batch in getLabels:\n",
    "    t = []\n",
    "    for b in batch:\n",
    "        t.append(np.argmax(b))\n",
    "    finalLabels.append(t)\n",
    "\n",
    "\n",
    "finalLabels = np.array(finalLabels)\n",
    "predictions = list(predictions)\n",
    "\n",
    "finalLabelss = []\n",
    "for f in finalLabels:\n",
    "    for d in f:\n",
    "        finalLabelss.append(d)\n",
    "\n",
    "predictionss = []\n",
    "for f in predictions:\n",
    "    for d in f:\n",
    "        predictionss.append(d)\n",
    "\n",
    "print(classification_report(finalLabelss, predictionss, target_names = [\"1\",\"2\",\"3\"]))\n",
    "rnnacc = classification_report(finalLabelss, predictionss, target_names = [\"1\",\"2\",\"3\"], output_dict = True)\n",
    "print('Final accuracy', rnnacc['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc4f50",
   "metadata": {},
   "source": [
    "#### Q) What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models.\n",
    "#### Feedforward neural network model part (a) has higher accuracy compare to RNN model, this can be because of the reason,\n",
    "#### 1) FNN models are generally faster to train and require fewer computational resources compared to RNN models.\n",
    "#### 2) Amazon reviews are often relatively short in length, and the context of the review can be inferred from a small window of words. FNN models are particularly good at capturing local features, as they can detect patterns in the input data regardless of their position in the sequence. In contrast, RNN models require contextual information from the entire sequence to make predictions, which may be less effective for short reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb82583",
   "metadata": {},
   "source": [
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ffc976",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b968a9",
   "metadata": {},
   "source": [
    "#### This is a PyTorch implementation of a GRU model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "746bef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRU, self).__init__()\n",
    "        self.rnn = nn.GRU(300, 20, batch_first = True)   \n",
    "        self.fc = nn.Linear(20, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 20,300)\n",
    "        x, hidden = self.rnn(x)\n",
    "        x = self.fc(x[:,-1,:])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b43f2bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gruModel = GRU().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(gruModel.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "15acf8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training loss 0.9509589121863246\n",
      "Validation loss: 0.862790741611893\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 2\n",
      "Training loss 0.821629304054038\n",
      "Validation loss: 0.8170948574164262\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 3\n",
      "Training loss 0.7905102463557074\n",
      "Validation loss: 0.8015320710120722\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 4\n",
      "Training loss 0.7728721328392469\n",
      "Validation loss: 0.7912074330394777\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 5\n",
      "Training loss 0.7598668912637513\n",
      "Validation loss: 0.7940821511450534\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 6\n",
      "Training loss 0.7506549125196568\n",
      "Validation loss: 0.7867725321394391\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 7\n",
      "Training loss 0.744831277526333\n",
      "Validation loss: 0.7787708392116862\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 8\n",
      "Training loss 0.7376943954521945\n",
      "Validation loss: 0.7797127753527214\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 9\n",
      "Training loss 0.73214642860267\n",
      "Validation loss: 0.7848938485740218\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 10\n",
      "Training loss 0.7283473157615518\n",
      "Validation loss: 0.7788904725195219\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 11\n",
      "Training loss 0.7229536588178986\n",
      "Validation loss: 0.7820751025088442\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 12\n",
      "Training loss 0.7201826189439938\n",
      "Validation loss: 0.7905023063841121\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 13\n",
      "Training loss 0.7176107353549644\n",
      "Validation loss: 0.77641498746591\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 14\n",
      "Training loss 0.7115796215914888\n",
      "Validation loss: 0.7829662382047778\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 15\n",
      "Training loss 0.7089325097831509\n",
      "Validation loss: 0.7843103316456546\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 16\n",
      "Training loss 0.7067036136218813\n",
      "Validation loss: 0.776640444817332\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 17\n",
      "Training loss 0.7025571980116267\n",
      "Validation loss: 0.7829716606948836\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 18\n",
      "Training loss 0.7007184708138229\n",
      "Validation loss: 0.794250080226843\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 19\n",
      "Training loss 0.6978250356219942\n",
      "Validation loss: 0.7829744406542741\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 20\n",
      "Training loss 0.696156366439963\n",
      "Validation loss: 0.7912755270663959\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 21\n",
      "Training loss 0.694185428873112\n",
      "Validation loss: 0.7857230496371631\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 22\n",
      "Training loss 0.6935284694030222\n",
      "Validation loss: 0.7843030978925526\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 23\n",
      "Training loss 0.6896990545365843\n",
      "Validation loss: 0.7868847557546105\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 24\n",
      "Training loss 0.6880047719456877\n",
      "Validation loss: 0.7923771696188487\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 25\n",
      "Training loss 0.6853798268696215\n",
      "Validation loss: 0.7962093706831026\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 26\n",
      "Training loss 0.683648334067606\n",
      "Validation loss: 0.795470546253724\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 27\n",
      "Training loss 0.6843046351003383\n",
      "Validation loss: 0.7922210199990465\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 28\n",
      "Training loss 0.6799717093650057\n",
      "Validation loss: 0.7982499174338687\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 29\n",
      "Training loss 0.6790388813133759\n",
      "Validation loss: 0.791440792894379\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 30\n",
      "Training loss 0.677948063762063\n",
      "Validation loss: 0.7923839490424919\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 31\n",
      "Training loss 0.6783095939920556\n",
      "Validation loss: 0.8008777759385606\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 32\n",
      "Training loss 0.6737102895490049\n",
      "Validation loss: 0.7962363455869879\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 33\n",
      "Training loss 0.6764701716813821\n",
      "Validation loss: 0.7985212057894872\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 34\n",
      "Training loss 0.6732057865424722\n",
      "Validation loss: 0.7996840279051103\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 35\n",
      "Training loss 0.6749009423347501\n",
      "Validation loss: 0.7983069888200843\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 36\n",
      "Training loss 0.6702654736034844\n",
      "Validation loss: 0.8043730773200126\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 37\n",
      "Training loss 0.6700420500676555\n",
      "Validation loss: 0.8034402681100958\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 38\n",
      "Training loss 0.66748624295427\n",
      "Validation loss: 0.8073292122407971\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 39\n",
      "Training loss 0.6679255194691359\n",
      "Validation loss: 0.805103863760441\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 40\n",
      "Training loss 0.6683073002135691\n",
      "Validation loss: 0.8083447654509218\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 41\n",
      "Training loss 0.6689788864672398\n",
      "Validation loss: 0.809363223851736\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 42\n",
      "Training loss 0.667565333479802\n",
      "Validation loss: 0.8085030402855482\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 43\n",
      "Training loss 0.6648803572738058\n",
      "Validation loss: 0.8103755721179768\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 44\n",
      "Training loss 0.6643459064066992\n",
      "Validation loss: 0.8121080010496468\n",
      "____________________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45\n",
      "Training loss 0.6642197090392487\n",
      "Validation loss: 0.8081828854387859\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 46\n",
      "Training loss 0.6630920373023546\n",
      "Validation loss: 0.8088105841621291\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 47\n",
      "Training loss 0.6622333315980213\n",
      "Validation loss: 0.8158796303522928\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 48\n",
      "Training loss 0.6601958712139943\n",
      "Validation loss: 0.8086341318273916\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 49\n",
      "Training loss 0.6600044007096828\n",
      "Validation loss: 0.812993329585297\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 50\n",
      "Training loss 0.6575320607920778\n",
      "Validation loss: 0.8102143458936286\n",
      "____________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "runModel(gruModel, optimizer, criterion, trainLoader, testLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e1447b",
   "metadata": {},
   "source": [
    "#### load efficient model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e8675983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.64      0.62      0.63      3963\n",
      "           2       0.55      0.57      0.56      4014\n",
      "           3       0.72      0.72      0.72      4023\n",
      "\n",
      "    accuracy                           0.64     12000\n",
      "   macro avg       0.64      0.64      0.64     12000\n",
      "weighted avg       0.64      0.64      0.64     12000\n",
      "\n",
      "Final accuracy 0.6363333333333333\n"
     ]
    }
   ],
   "source": [
    "getPred = []\n",
    "getLabels = []\n",
    "\n",
    "gruModel.eval()\n",
    "with torch.no_grad():\n",
    "    for data, target in testLoader:\n",
    "        output = gruModel(data)\n",
    "        \n",
    "        getLabels.append(target.cpu().detach().numpy())\n",
    "        _, predicted = torch.max(output, 1) \n",
    "        getPred.append(predicted.cpu().detach().numpy())\n",
    "\n",
    "predictions = np.array(getPred)\n",
    "\n",
    "finalLabels = []\n",
    "for batch in getLabels:\n",
    "    t = []\n",
    "    for b in batch:\n",
    "        t.append(np.argmax(b))\n",
    "    finalLabels.append(t)\n",
    "    \n",
    "finalLabels = np.array(finalLabels)\n",
    "predictions = list(predictions)\n",
    "\n",
    "finalLabelss = []\n",
    "for f in finalLabels:\n",
    "    for d in f:\n",
    "        finalLabelss.append(d)\n",
    "\n",
    "predictionss = []\n",
    "for f in predictions:\n",
    "    for d in f:\n",
    "        predictionss.append(d)\n",
    "\n",
    "print(classification_report(finalLabelss, predictionss, target_names = [\"1\",\"2\",\"3\"]))\n",
    "gruacc = classification_report(finalLabelss, predictionss, target_names = [\"1\",\"2\",\"3\"], output_dict = True)\n",
    "print('Final accuracy', gruacc['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d1e132",
   "metadata": {},
   "source": [
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9fea04",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6035333",
   "metadata": {},
   "source": [
    "#### This is a PyTorch implementation of a LSTM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9c5da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(300, 20, batch_first=True)\n",
    "        self.fc = nn.Linear(20, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 20, 300)\n",
    "        x, (hidden, cell) = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a3e87b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmModel = LSTM().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(lstmModel.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "472bc296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training loss 0.9910731069346269\n",
      "Validation loss: 0.9463157417476177\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 2\n",
      "Training loss 0.927007671178629\n",
      "Validation loss: 0.9319850789209206\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 3\n",
      "Training loss 0.9155461088282366\n",
      "Validation loss: 0.9240030299127102\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 4\n",
      "Training loss 0.9105994057382146\n",
      "Validation loss: 0.9399468132654826\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 5\n",
      "Training loss 0.9104015912984809\n",
      "Validation loss: 0.9232918983449538\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 6\n",
      "Training loss 0.9046899565545221\n",
      "Validation loss: 0.9225457444985707\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 7\n",
      "Training loss 0.899113389633596\n",
      "Validation loss: 0.9257132970889409\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 8\n",
      "Training loss 0.8988818768970668\n",
      "Validation loss: 0.9195610113690297\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 9\n",
      "Training loss 0.8996269811366995\n",
      "Validation loss: 0.9177948820541303\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 10\n",
      "Training loss 0.8950750934717556\n",
      "Validation loss: 0.9164268413484097\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 11\n",
      "Training loss 0.8966066627192001\n",
      "Validation loss: 0.9133830656260252\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 12\n",
      "Training loss 0.8919673408841093\n",
      "Validation loss: 0.917684240748485\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 13\n",
      "Training loss 0.889865371628354\n",
      "Validation loss: 0.9165606890022755\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 14\n",
      "Training loss 0.8883052570770185\n",
      "Validation loss: 0.9141862967113654\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 15\n",
      "Training loss 0.882326505502065\n",
      "Validation loss: 0.9111490458001693\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 16\n",
      "Training loss 0.8788255847406884\n",
      "Validation loss: 0.9084852461616199\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 17\n",
      "Training loss 0.8777827304340899\n",
      "Validation loss: 0.9118316633651654\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 18\n",
      "Training loss 0.8758757400053243\n",
      "Validation loss: 0.90949314742287\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 19\n",
      "Training loss 0.8735786224429806\n",
      "Validation loss: 0.9105867040852705\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 20\n",
      "Training loss 0.8703553038785855\n",
      "Validation loss: 0.9096167406986158\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 21\n",
      "Training loss 0.8680790794243415\n",
      "Validation loss: 0.9109540701359511\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 22\n",
      "Training loss 0.8679915134149293\n",
      "Validation loss: 0.908227656647563\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 23\n",
      "Training loss 0.865448514038076\n",
      "Validation loss: 0.9115897860030333\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 24\n",
      "Training loss 0.8640191107653081\n",
      "Validation loss: 0.9111301306287448\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 25\n",
      "Training loss 0.8633042086760203\n",
      "Validation loss: 0.909008780837059\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 26\n",
      "Training loss 0.8645294316125413\n",
      "Validation loss: 0.9112595623483261\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 27\n",
      "Training loss 0.8604773094132543\n",
      "Validation loss: 0.9107933727850517\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 28\n",
      "Training loss 0.8629405195191503\n",
      "Validation loss: 0.9137267689903578\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 29\n",
      "Training loss 0.8622919616413613\n",
      "Validation loss: 0.9163985406508048\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 30\n",
      "Training loss 0.860788869065543\n",
      "Validation loss: 0.9111189749439558\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 31\n",
      "Training loss 0.8557872375051181\n",
      "Validation loss: 0.9106509808947643\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 32\n",
      "Training loss 0.8529513090935846\n",
      "Validation loss: 0.9109651161630948\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 33\n",
      "Training loss 0.8532394260937969\n",
      "Validation loss: 0.9086433684080839\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 34\n",
      "Training loss 0.8540289044889311\n",
      "Validation loss: 0.9136970843126376\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 35\n",
      "Training loss 0.8577141590143244\n",
      "Validation loss: 0.9137106612225374\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 36\n",
      "Training loss 0.8548214110756914\n",
      "Validation loss: 0.9127487608542045\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 37\n",
      "Training loss 0.8516405243823926\n",
      "Validation loss: 0.9116508712967237\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 38\n",
      "Training loss 0.8527343564281861\n",
      "Validation loss: 0.9118353686531385\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 39\n",
      "Training loss 0.8520192508573333\n",
      "Validation loss: 0.9101043612162272\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 40\n",
      "Training loss 0.852254587808003\n",
      "Validation loss: 0.9088120809644461\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 41\n",
      "Training loss 0.8460676958921055\n",
      "Validation loss: 0.9087160489012798\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 42\n",
      "Training loss 0.8443709157171349\n",
      "Validation loss: 0.9096598298847676\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 43\n",
      "Training loss 0.8462240137246748\n",
      "Validation loss: 0.910176565999786\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 44\n",
      "Training loss 0.8449317277061442\n",
      "Validation loss: 0.9103226556330919\n",
      "____________________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45\n",
      "Training loss 0.8509866646205385\n",
      "Validation loss: 0.9123913323730231\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 46\n",
      "Training loss 0.8479025286460916\n",
      "Validation loss: 0.911289652556181\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 47\n",
      "Training loss 0.8440343400314451\n",
      "Validation loss: 0.9104504268368085\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 48\n",
      "Training loss 0.8429608829095959\n",
      "Validation loss: 0.9139871908773979\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 49\n",
      "Training loss 0.8436000204309821\n",
      "Validation loss: 0.9137538412859042\n",
      "____________________________________________________________________________________________________________\n",
      "Epoch: 50\n",
      "Training loss 0.8403737978463371\n",
      "Validation loss: 0.9080070976714293\n",
      "____________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "runModel(lstmModel, optimizer, criterion, trainLoader, testLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ed35f",
   "metadata": {},
   "source": [
    "#### load efficient model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "136d8853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.62      0.63      3963\n",
      "           2       0.56      0.55      0.55      4014\n",
      "           3       0.70      0.73      0.72      4023\n",
      "\n",
      "    accuracy                           0.63     12000\n",
      "   macro avg       0.63      0.63      0.63     12000\n",
      "weighted avg       0.63      0.63      0.63     12000\n",
      "\n",
      "Final accuracy 0.63375\n"
     ]
    }
   ],
   "source": [
    "getPred = []\n",
    "getLabels = []\n",
    "\n",
    "lstmModel.eval()\n",
    "with torch.no_grad():\n",
    "    for data, target in testLoader:\n",
    "        output = lstmModel(data)\n",
    "        \n",
    "        getLabels.append(target.cpu().detach().numpy())\n",
    "        _, predicted = torch.max(output, 1) \n",
    "        getPred.append(predicted.cpu().detach().numpy())\n",
    "\n",
    "predictions = np.array(getPred)\n",
    "\n",
    "finalLabels = []\n",
    "for batch in getLabels:\n",
    "    t = []\n",
    "    for b in batch:\n",
    "        t.append(np.argmax(b))\n",
    "    finalLabels.append(t)\n",
    "\n",
    "\n",
    "finalLabels = np.array(finalLabels)\n",
    "predictions = list(predictions)\n",
    "\n",
    "finalLabelss = []\n",
    "for f in finalLabels:\n",
    "    for d in f:\n",
    "        finalLabelss.append(d)\n",
    "\n",
    "predictionss = []\n",
    "for f in predictions:\n",
    "    for d in f:\n",
    "        predictionss.append(d)\n",
    "\n",
    "print(classification_report(finalLabelss, predictionss, target_names = [\"1\",\"2\",\"3\"]))\n",
    "lstmacc = classification_report(finalLabelss, predictionss, target_names = [\"1\",\"2\",\"3\"], output_dict = True)\n",
    "print('Final accuracy', lstmacc['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d1276",
   "metadata": {},
   "source": [
    "#### Q) What do you conclude by comparing accuracy values you obtain by GRU, LSTM, and simple RNN\n",
    "#### The accuracy scores achieved with GRU are slightly greater to those achieved with RNN and LSTM. This is due to the gates present in GRU, which effectively address the issue of vanishing gradients during backpropagation, allowing for more effective loss propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e0cac33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each Model accuracy\n",
      "TF-IDF Perceptron: 0.504\n",
      "TF-IDF SVM: 0.6976666666666667\n",
      "Word2Vec Perceptron: 0.504\n",
      "Word2Vec SVM: 0.6746666666666666\n",
      "Feedforwad neural network part(a): 0.6928333333333333\n",
      "Feedforwad neural network part(b): 0.5068333333333334\n",
      "RNN: 0.6274166666666666\n",
      "GRU: 0.6363333333333333\n",
      "LSTM: 0.63375\n"
     ]
    }
   ],
   "source": [
    "print('Each Model accuracy')\n",
    "print(\"TF-IDF Perceptron:\",perceptronCR['accuracy'])\n",
    "print(\"TF-IDF SVM:\",tfidfsvmacc['accuracy'])\n",
    "print(\"Word2Vec Perceptron:\",w2vpacc['accuracy'])\n",
    "print(\"Word2Vec SVM:\",w2vsvmacc['accuracy'])\n",
    "print(\"Feedforwad neural network part(a):\",fnnacc['accuracy'])\n",
    "print(\"Feedforwad neural network part(b):\",fnnconacc['accuracy'])\n",
    "print(\"RNN:\",rnnacc['accuracy'])\n",
    "print(\"GRU:\",gruacc['accuracy'])\n",
    "print(\"LSTM:\",lstmacc['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d238dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
